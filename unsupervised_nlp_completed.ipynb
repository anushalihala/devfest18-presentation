{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read movie titles, 100 movies; the last one is empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = open('data/title_list.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Godfather',\n",
       " 'The Shawshank Redemption',\n",
       " \"Schindler's List\",\n",
       " 'Raging Bull',\n",
       " 'Casablanca',\n",
       " \"One Flew Over the Cuckoo's Nest\",\n",
       " 'Gone with the Wind',\n",
       " 'Citizen Kane',\n",
       " 'The Wizard of Oz',\n",
       " 'Titanic']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles[:100]\n",
    "# Just turning it around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Genres information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = open('data/genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[u' Crime', u' Drama']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the synopses from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_wiki = open('data/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synopses_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_wiki = synopses_wiki[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses_wiki[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strips html formatting and converts to unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_clean_wiki = []\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    synopses_clean_wiki.append(text)\n",
    "synopses_wiki = synopses_clean_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses_wiki[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read synopses information from imdb, which might be different from wiki. Also cleaned as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_imdb = open('data/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    synopses_clean_imdb.append(text)\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses_imdb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine synopses from wiki and imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses = []\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 titles\n",
      "100 genres\n",
      "100 synopses\n"
     ]
    }
   ],
   "source": [
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(genres)) + ' genres')\n",
    "print(str(len(synopses)) + ' synopses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
    "# the movies in the list are already ranked from 1 to 100\n",
    "ranks = []\n",
    "for i in range(1, len(titles)+1):\n",
    "    ranks.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "# use nltk.download() to install the corpus first\n",
    "# Stop Words are words which do not contain important significance to be used in Search Queries\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent for sent in nltk.sent_tokenize(\"Today (May 19, 2016) is his only daughter's wedding. Vito Corleone is the Godfather. Vito's youngest son, Michael, in a Marine Corps uniform, introduces his girlfriend, Kay Adams, to his family at the sprawling reception.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Today (May 19, 2016) is his only daughter's wedding.\",\n",
       " 'Vito Corleone is the Godfather.',\n",
       " \"Vito's youngest son, Michael, in a Marine Corps uniform, introduces his girlfriend, Kay Adams, to his family at the sprawling reception.\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " '(',\n",
       " 'May',\n",
       " '19',\n",
       " ',',\n",
       " '2016',\n",
       " ')',\n",
       " 'is',\n",
       " 'his',\n",
       " 'only',\n",
       " 'daughter',\n",
       " \"'s\",\n",
       " 'wedding',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in nltk.word_tokenize(sents[0])]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            filtered_words.append(word)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how \"only\" is stemmed to \"onli\" and \"wedding\" is stemmed to \"wed\"\n",
    "stems = [stemmer.stem(t) for t in filtered_words]\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "# Punkt Sentence Tokenizer, sent means sentence \n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']\n"
     ]
    }
   ],
   "source": [
    "words_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "print(words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'may', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_only = tokenize_only(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "words_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I use my stemming/tokenizing and tokenizing functions to iterate over the list of synopses to create two vocabularies: one stemmed and one only tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312301\n",
      "312301\n"
     ]
    }
   ],
   "source": [
    "print(len(totalvocab_stemmed))\n",
    "print(len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 312301 items in vocab_frame\n",
      "     words\n",
      "plot  plot\n",
      "edit  edit\n",
      "edit  edit\n",
      "edit  edit\n",
      "on      on\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate TF-IDF matrix (see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 563)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the result of this block takes a while to show\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "# (100, 563) means the matrix has 100 rows and 563 columns\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the fun part. Using the tf-idf matrix, you can run a slew of clustering algorithms to better understand the hidden structure within the synopses. I first chose k-means. K-means initializes with a pre-determined number of clusters (I chose 5). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence.\n",
    "\n",
    "I found it took several runs for the algorithm to converge a global optimum as k-means is susceptible to reaching local optima - how to decide that the algorithm converged???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 827 ms, sys: 0 ns, total: 827 ms\n",
      "Wall time: 863 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a dictionary of titles, ranks, the synopsis, the cluster assignment, and the genre [rank and genre were scraped from IMDB].\n",
    "I convert this dictionary to a Pandas DataFrame for easy access. I'm a huge fan of Pandas and recommend taking a look at some of its awesome functionality which I'll use below, but not describe in a ton of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rank                                              title  cluster  \\\n",
      "1      1                                      The Godfather        1   \n",
      "3      2                           The Shawshank Redemption        3   \n",
      "2      3                                   Schindler's List        2   \n",
      "1      4                                        Raging Bull        1   \n",
      "1      5                                         Casablanca        1   \n",
      "2      6                    One Flew Over the Cuckoo's Nest        2   \n",
      "1      7                                 Gone with the Wind        1   \n",
      "1      8                                       Citizen Kane        1   \n",
      "3      9                                   The Wizard of Oz        3   \n",
      "3     10                                            Titanic        3   \n",
      "2     11                                 Lawrence of Arabia        2   \n",
      "1     12                             The Godfather: Part II        1   \n",
      "3     13                                             Psycho        3   \n",
      "1     14                                       Sunset Blvd.        1   \n",
      "3     15                                            Vertigo        3   \n",
      "1     16                                  On the Waterfront        1   \n",
      "3     17                                       Forrest Gump        3   \n",
      "0     18                                 The Sound of Music        0   \n",
      "1     19                                    West Side Story        1   \n",
      "2     20                                          Star Wars        2   \n",
      "3     21                         E.T. the Extra-Terrestrial        3   \n",
      "3     22                              2001: A Space Odyssey        3   \n",
      "3     23                           The Silence of the Lambs        3   \n",
      "3     24                                          Chinatown        3   \n",
      "0     25                       The Bridge on the River Kwai        0   \n",
      "1     26                                Singin' in the Rain        1   \n",
      "4     27                              It's a Wonderful Life        4   \n",
      "1     28                                   Some Like It Hot        1   \n",
      "1     29                                       12 Angry Men        1   \n",
      "0     30  Dr. Strangelove or: How I Learned to Stop Worr...        0   \n",
      "..   ...                                                ...      ...   \n",
      "1     71                                           Rain Man        1   \n",
      "1     72                                         Annie Hall        1   \n",
      "1     73                                      Out of Africa        1   \n",
      "3     74                                  Good Will Hunting        3   \n",
      "1     75                                Terms of Endearment        1   \n",
      "4     76                                            Tootsie        4   \n",
      "3     77                                              Fargo        3   \n",
      "1     78                                              Giant        1   \n",
      "2     79                                The Grapes of Wrath        2   \n",
      "2     80                                              Shane        2   \n",
      "3     81                                     The Green Mile        3   \n",
      "3     82                 Close Encounters of the Third Kind        3   \n",
      "3     83                                            Network        3   \n",
      "1     84                                          Nashville        1   \n",
      "3     85                                       The Graduate        3   \n",
      "3     86                                  American Graffiti        3   \n",
      "3     87                                       Pulp Fiction        3   \n",
      "0     88                                  The African Queen        0   \n",
      "2     89                                         Stagecoach        2   \n",
      "0     90                               Mutiny on the Bounty        0   \n",
      "3     91                                 The Maltese Falcon        3   \n",
      "3     92                                 A Clockwork Orange        3   \n",
      "3     93                                        Taxi Driver        3   \n",
      "1     94                                  Wuthering Heights        1   \n",
      "3     95                                   Double Indemnity        3   \n",
      "3     96                              Rebel Without a Cause        3   \n",
      "3     97                                        Rear Window        3   \n",
      "3     98                                      The Third Man        3   \n",
      "3     99                                 North by Northwest        3   \n",
      "1    100                                Yankee Doodle Dandy        1   \n",
      "\n",
      "                                                genre  \n",
      "1                              [u' Crime', u' Drama']  \n",
      "3                              [u' Crime', u' Drama']  \n",
      "2             [u' Biography', u' Drama', u' History']  \n",
      "1               [u' Biography', u' Drama', u' Sport']  \n",
      "1                   [u' Drama', u' Romance', u' War']  \n",
      "2                                         [u' Drama']  \n",
      "1                   [u' Drama', u' Romance', u' War']  \n",
      "1                            [u' Drama', u' Mystery']  \n",
      "3   [u' Adventure', u' Family', u' Fantasy', u' Mu...  \n",
      "3                            [u' Drama', u' Romance']  \n",
      "2   [u' Adventure', u' Biography', u' Drama', u' H...  \n",
      "1                              [u' Crime', u' Drama']  \n",
      "3             [u' Horror', u' Mystery', u' Thriller']  \n",
      "1                          [u' Drama', u' Film-Noir']  \n",
      "3            [u' Mystery', u' Romance', u' Thriller']  \n",
      "1                              [u' Crime', u' Drama']  \n",
      "3                            [u' Drama', u' Romance']  \n",
      "0   [u' Biography', u' Drama', u' Family', u' Musi...  \n",
      "1   [u' Crime', u' Drama', u' Musical', u' Romance...  \n",
      "2   [u' Action', u' Adventure', u' Fantasy', u' Sc...  \n",
      "3             [u' Adventure', u' Family', u' Sci-Fi']  \n",
      "3                           [u' Mystery', u' Sci-Fi']  \n",
      "3                [u' Crime', u' Drama', u' Thriller']  \n",
      "3              [u' Drama', u' Mystery', u' Thriller']  \n",
      "0                 [u' Adventure', u' Drama', u' War']  \n",
      "1              [u' Comedy', u' Musical', u' Romance']  \n",
      "4                [u' Drama', u' Family', u' Fantasy']  \n",
      "1                                        [u' Comedy']  \n",
      "1                                         [u' Drama']  \n",
      "0                               [u' Comedy', u' War']  \n",
      "..                                                ...  \n",
      "1                                         [u' Drama']  \n",
      "1                [u' Comedy', u' Drama', u' Romance']  \n",
      "1             [u' Biography', u' Drama', u' Romance']  \n",
      "3                                         [u' Drama']  \n",
      "1                             [u' Comedy', u' Drama']  \n",
      "4                [u' Comedy', u' Drama', u' Romance']  \n",
      "3                [u' Crime', u' Drama', u' Thriller']  \n",
      "1                            [u' Drama', u' Romance']  \n",
      "2                                         [u' Drama']  \n",
      "2               [u' Drama', u' Romance', u' Western']  \n",
      "3    [u' Crime', u' Drama', u' Fantasy', u' Mystery']  \n",
      "3                             [u' Drama', u' Sci-Fi']  \n",
      "3                                         [u' Drama']  \n",
      "1                              [u' Drama', u' Music']  \n",
      "3                [u' Comedy', u' Drama', u' Romance']  \n",
      "3                             [u' Comedy', u' Drama']  \n",
      "3                [u' Crime', u' Drama', u' Thriller']  \n",
      "0               [u' Adventure', u' Romance', u' War']  \n",
      "2                        [u' Adventure', u' Western']  \n",
      "0             [u' Adventure', u' Drama', u' History']  \n",
      "3             [u' Drama', u' Film-Noir', u' Mystery']  \n",
      "3                  [u' Crime', u' Drama', u' Sci-Fi']  \n",
      "3                              [u' Crime', u' Drama']  \n",
      "1                            [u' Drama', u' Romance']  \n",
      "3   [u' Crime', u' Drama', u' Film-Noir', u' Thril...  \n",
      "3                                         [u' Drama']  \n",
      "3                         [u' Mystery', u' Thriller']  \n",
      "3          [u' Film-Noir', u' Mystery', u' Thriller']  \n",
      "3                         [u' Mystery', u' Thriller']  \n",
      "1             [u' Biography', u' Drama', u' Musical']  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    34\n",
       "3    30\n",
       "2    25\n",
       "0     6\n",
       "4     5\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "\n",
    "print(frame) # here the ranking is still 0 to 99\n",
    "\n",
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that clusters 4 and 0 have the lowest rank, which indicates that they, on average, contain films that were ranked as \"better\" on the top 100 list.\n",
    "Here is some fancy indexing and sorting on each cluster to identify which are the top n (I chose n=6) words that are nearest to the cluster centroid. This gives a good sense of the main topic of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0 words: b'captain', b'office', b'command', b'water', b'mr.', b'strike',\n",
      "\n",
      "Cluster 0 titles: The Sound of Music, The Bridge on the River Kwai, Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, Apocalypse Now, The African Queen, Mutiny on the Bounty,\n",
      "\n",
      "Cluster 1 words: b'father', b'love', b'family', b'film', b'new', b'marries',\n",
      "\n",
      "Cluster 1 titles: The Godfather, Raging Bull, Casablanca, Gone with the Wind, Citizen Kane, The Godfather: Part II, Sunset Blvd., On the Waterfront, West Side Story, Singin' in the Rain, Some Like It Hot, 12 Angry Men, Amadeus, Gandhi, Rocky, A Streetcar Named Desire, To Kill a Mockingbird, An American in Paris, The Best Years of Our Lives, My Fair Lady, Doctor Zhivago, High Noon, Goodfellas, City Lights, It Happened One Night, Midnight Cowboy, Rain Man, Annie Hall, Out of Africa, Terms of Endearment, Giant, Nashville, Wuthering Heights, Yankee Doodle Dandy,\n",
      "\n",
      "Cluster 2 words: b'killed', b'soldiers', b'army', b'orders', b'men', b'camps',\n",
      "\n",
      "Cluster 2 titles: Schindler's List, One Flew Over the Cuckoo's Nest, Lawrence of Arabia, Star Wars, The Lord of the Rings: The Return of the King, Gladiator, From Here to Eternity, Saving Private Ryan, Unforgiven, Raiders of the Lost Ark, Patton, Jaws, Braveheart, The Good, the Bad and the Ugly, Butch Cassidy and the Sundance Kid, The Treasure of the Sierra Madre, Platoon, Dances with Wolves, The Pianist, The Deer Hunter, All Quiet on the Western Front, Mr. Smith Goes to Washington, The Grapes of Wrath, Shane, Stagecoach,\n",
      "\n",
      "Cluster 3 words: b'police', b'car', b'mother', b'apartments', b'house', b'driving',\n",
      "\n",
      "Cluster 3 titles: The Shawshank Redemption, The Wizard of Oz, Titanic, Psycho, Vertigo, Forrest Gump, E.T. the Extra-Terrestrial, 2001: A Space Odyssey, The Silence of the Lambs, Chinatown, Ben-Hur, The Apartment, The Exorcist, The French Connection, Good Will Hunting, Fargo, The Green Mile, Close Encounters of the Third Kind, Network, The Graduate, American Graffiti, Pulp Fiction, The Maltese Falcon, A Clockwork Orange, Taxi Driver, Double Indemnity, Rebel Without a Cause, Rear Window, The Third Man, North by Northwest,\n",
      "\n",
      "Cluster 4 words: b'george', b'marries', b'family', b'john', b'james', b'playing',\n",
      "\n",
      "Cluster 4 titles: It's a Wonderful Life, The Philadelphia Story, The King's Speech, A Place in the Sun, Tootsie,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus (each synopsis among the synopses). cosine similarity 1 means the same document, 0 means totally different ones. dist is defined as 1 - the cosine similarity of each document.  Subtracting it from 1 provides cosine distance which I will use for plotting on a euclidean (2-dimensional) plane.\n",
    "Note that with dist it is possible to evaluate the similarity of any two or more synopses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "print(type(similarity_distance))\n",
    "print(similarity_distance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Multidimensional scaling\n",
    "Here is some code to convert the dist matrix into a 2-dimensional array using multidimensional scaling. I won't pretend I know a ton about MDS, but it was useful for this purpose. Another option would be to use principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergings = linkage(similarity_distance, method='complete')\n",
    "\n",
    "# Plot the dendrogram, using titles as labels\n",
    "dendrogram(mergings,\n",
    "           labels=titles,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=16,\n",
    ")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(108, 21)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
