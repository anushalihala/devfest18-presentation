{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read movie titles, 100 movies; the last one is empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = open('data/title_list.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles[:100]\n",
    "# Just turning it around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Genres information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = open('data/genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the synopses from wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_wiki = open('data/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(synopses_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_wiki = synopses_wiki[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses_wiki[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strips html formatting and converts to unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_clean_wiki = []\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    synopses_clean_wiki.append(text)\n",
    "synopses_wiki = synopses_clean_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses_wiki[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read synopses information from imdb, which might be different from wiki. Also cleaned as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses_imdb = open('data/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    synopses_clean_imdb.append(text)\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses_imdb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine synopses from wiki and imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopses = []\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synopses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(genres)) + ' genres')\n",
    "print(str(len(synopses)) + ' synopses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
    "# the movies in the list are already ranked from 1 to 100\n",
    "ranks = []\n",
    "for i in range(1, len(titles)+1):\n",
    "    ranks.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "# use nltk.download() to install the corpus first\n",
    "# Stop Words are words which do not contain important significance to be used in Search Queries\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = [sent for sent in nltk.sent_tokenize(\"Today (May 19, 2016) is his only daughter's wedding. Vito Corleone is the Godfather. Vito's youngest son, Michael, in a Marine Corps uniform, introduces his girlfriend, Kay Adams, to his family at the sprawling reception.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents into sentences\n",
    "\n",
    "#Create a variable 'sents' which is a list of strings obtained by tokenizing the string below:\n",
    "'''\n",
    "Today (May 19, 2016) is his only daughter's wedding. Vito Corleone is the Godfather. Vito's youngest son, Michael, in a Marine Corps uniform, introduces his girlfriend, Kay Adams, to his family at the sprawling reception.\n",
    "'''\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences into words\n",
    "\n",
    "#Create a variable 'words' which is a list of strings obtained by tokenizing the first string in sents \n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            filtered_words.append(word)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words \n",
    "\n",
    "#Create a variable 'stems' which is a list of stemmed words obtained by stemming filtered_words\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how \"only\" is stemmed to \"onli\" and \"wedding\" is stemmed to \"wed\"\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "# Punkt Sentence Tokenizer, sent means sentence \n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "print(words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_only = tokenize_only(\"Today (May 19, 2016) is his only daughter's wedding.\")\n",
    "words_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I use my stemming/tokenizing and tokenizing functions to iterate over the list of synopses to create two vocabularies: one stemmed and one only tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(totalvocab_stemmed))\n",
    "print(len(totalvocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate TF-IDF matrix (see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that the result of this block takes a while to show\n",
    "\n",
    "#Import TfidfVectorizer from sklearn.feature_extraction.text\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "\n",
    "\n",
    "#define vectorizer parameters\n",
    "\n",
    "# Create a TfidfVectorizer named tfidf_vectorizer with the below configuration\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "__________ = __________(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=_______, ngram_range=(_, _))\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "\n",
    "#fit transform synopses to tfidf_vectorizer and store as tfidf_matrix\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "# (100, 563) means the matrix has 100 rows and 563 columns\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the fun part. Using the tf-idf matrix, you can run a slew of clustering algorithms to better understand the hidden structure within the synopses. I first chose k-means. K-means initializes with a pre-determined number of clusters (I chose 5). Each observation is assigned to a cluster (cluster assignment) so as to minimize the within cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence.\n",
    "\n",
    "I found it took several runs for the algorithm to converge a global optimum as k-means is susceptible to reaching local optima - how to decide that the algorithm converged???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import KMeans from sklearn.cluster\n",
    "#create KMeans object km with n_clusters = 5\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "#fit tfidf_matrix to km \n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I create a dictionary of titles, ranks, the synopsis, the cluster assignment, and the genre [rank and genre were scraped from IMDB].\n",
    "I convert this dictionary to a Pandas DataFrame for easy access. I'm a huge fan of Pandas and recommend taking a look at some of its awesome functionality which I'll use below, but not describe in a ton of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "\n",
    "print(frame) # here the ranking is still 0 to 99\n",
    "\n",
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that clusters 4 and 0 have the lowest rank, which indicates that they, on average, contain films that were ranked as \"better\" on the top 100 list.\n",
    "Here is some fancy indexing and sorting on each cluster to identify which are the top n (I chose n=6) words that are nearest to the cluster centroid. This gives a good sense of the main topic of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.loc[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus (each synopsis among the synopses). cosine similarity 1 means the same document, 0 means totally different ones. dist is defined as 1 - the cosine similarity of each document.  Subtracting it from 1 provides cosine distance which I will use for plotting on a euclidean (2-dimensional) plane.\n",
    "Note that with dist it is possible to evaluate the similarity of any two or more synopses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cosine_similarity from sklearn.metrics.pairwise\n",
    "\n",
    "#calculate cosine_similarity with argument tfidf_matrix and store in variable cs \n",
    "\n",
    "#calculate similarity_distance as 1 - cs\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt and configure it to output inline\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import linkage and dendrogram from scipy.cluster.hierarchy\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "##########   CODE ABOVE    ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mergings as an linkage object over similarity_distance.\n",
    "\n",
    "##########   CODE BELOW    ##########\n",
    "\n",
    "\n",
    "\n",
    "# Plot the dendrogram, using titles as labels\n",
    "dendrogram(mergings,\n",
    "           labels=_____,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=16,\n",
    ")\n",
    "\n",
    "##########   CODE ABOVE    ##########\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(108, 21)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's All Fox!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
